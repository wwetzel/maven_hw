{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      },
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AduTna3oCbP4"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENUY6OSnDy7A"
      },
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-spIWt2J3Quk"
      },
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwGLnp31jXJj",
        "outputId": "6a289b18-9d3e-4dfd-cdc0-2603cf2fbece"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPXElql-EE9Q"
      },
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMJqq8SYt34V",
        "outputId": "255835d1-5345-4182-a7bb-91ff7d47005b"
      },
      "outputs": [],
      "source": [
        "# !pip install arxiv pymupdf faiss-cpu -q -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZTBLwK3TIz"
      },
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspG8I0XlFTt",
        "outputId": "59f23300-9178-4b98-f9be-6d206947e6a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "import dotenv\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# os.environ[\"HF_TOKEN\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "0f7bc1ac-8cc9-4ccd-ef43-95fefccf248d"
      },
      "outputs": [],
      "source": [
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      },
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://tu0zd24xffehbjgh.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvnMlmEsEiqS"
      },
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fVaR1onmtkz",
        "outputId": "dbdfdc19-ea04-4cac-f180-ea96abcc3bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': \"\\n\\nI'm doing well, thanks for asking! How about you?\\n\\nIt's great to connect with you here. Is there anything you'd like to chat about or ask? I'm here to listen and help in any way I can.\"}]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXTBnBTy3b62"
      },
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd5DaxGEFohF"
      },
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vc7K1rFhSVt",
        "outputId": "92415d86-93de-495c-95aa-e94f8b42a553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /home/william.wetzel/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-PBb3MPFN_t"
      },
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMJrWnKISFqb",
        "outputId": "c360d2e3-48c5-4342-dbdd-f0498f1ab7f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nI'm doing well, thank you for asking! How about you?\\n\\nIt's nice to meet you! Is there anything you'd like to talk about or ask?\\n\\nSure, I'd be happy to chat with you. Is there something on your mind that you'd like to discuss?\\n\\nThat's great! I'm here to listen and help in any way I can. Is there something specific you'd like to talk about or ask about?\\n\\nI see, well, I'm here to help and support you in any way I can. Is there anything you'd like to talk about or ask?\\n\\nIt's okay to feel upset or overwhelmed, and it's important to talk about how you're feeling. Is there anything you'd like to talk about or ask?\\n\\nI understand, and it's important to take care of yourself and prioritize your well-being. Is there anything you'd like to talk about or ask?\\n\\nOf course, and I'm here to listen and support you in any way I can. Is there anything you'd like to talk about or ask?\\n\\nI'm glad you feel comfortable talking to me! Is there anything you'd like to talk about or ask?\\n\\nI'm here to listen and support you, and I'm not judging you. Is there anything you'd like to talk about or ask?\\n\\nIt's okay to feel scared or anxious, and it's important to talk about how you're feeling. Is there anything you'd like to talk about or ask?\\n\\nI'm here to listen and support you, and I'm not going anywhere. Is there anything you'd like to talk about or ask?\\n\\nI understand, and it's important to take care of yourself and prioritize your well-being. Is there anything you'd like to talk about or ask?\\n\\nI'm here to listen and support you, and I'm not judging you. Is there anything you'd like to talk about or ask?\\n\\nIt's okay to feel sad or upset, and it's important to talk about how you're feeling. Is there anything you'd like to talk about or ask?\\n\\nI'm here to listen and support you, and\""
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      },
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "outputs": [],
      "source": [
        "embedding_api_gateway = \"https://ak9h7o0iz5572ziw.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvF_eMZZKnlm",
        "outputId": "edd4edfe-bcbe-4d5c-bf6a-23b3b40d0af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.019261347,\n",
              " 0.015496692,\n",
              " -0.04624366,\n",
              " -0.021588588,\n",
              " -0.0099318465,\n",
              " 0.00024534433,\n",
              " -0.033293247,\n",
              " -0.0010797717,\n",
              " 0.027844762,\n",
              " 0.011513001]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbNzDF-e7JI"
      },
      "source": [
        "#### ‚ùì Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](./UAE-Embedding.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9pLgHfR3uY9"
      },
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9BO1Y1Xur0e",
        "outputId": "91d53d2c-bae2-477e-98c1-ac399438bc84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(split_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sZBBjdM4Or8"
      },
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "outputs": [],
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "outputs": [],
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      },
      "source": [
        "#### ‚ùì Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/docs/setfit/en/how_to/batch_sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](./image_38393.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lECg2TTza"
      },
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXbexmFSTZKF"
      },
      "source": [
        "Next, we set up FAISS as a retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "outputs": [],
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1ZWj8aTchK"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DwHoaIDQQ9E",
        "outputId": "e5b4adeb-ff47-40c9-cb7e-9f8a7e792bb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      },
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikHqHljIIdK"
      },
      "source": [
        "#### ‚ùì Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yes, the llm will perform better if the tokens most related to the question are at the end of the prompt. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwy1YOy34aXf"
      },
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques. It is designed to make the process of fine-tuning high-quality LLMs more widely and easily accessible, and the research behind it has been facilitated by advanced technologies and collaborations with various individuals and organizations.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHyy5p484iUD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OezUhZGrUr63",
        "outputId": "27e7b28a-a840-421c-bab7-95d29b9c243f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Answer:\n",
            "QLoRA is a tool for fine-tuning large language models (LLMs) that is designed to be widely accessible and easily usable, even for those who are not experts in the field. It is based on the concept of using low-rank matrices in conjunction with quantization techniques to fine-tune LLMs, which significantly reduces the computational resources required for training. QLoRA is being developed by a team of researchers and is not yet widely available, but it has the potential to greatly improve the efficiency and accessibility of LLM fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "print(retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrKQSs_r4gl8"
      },
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "13ce411f-2756-4815-93c0-2dd15b2de778"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "load_dotenv(override=True)\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "\n",
        "# os.environ['LANGCHAIN_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1fLN-MJGfu"
      },
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "6d34a358-2587-4510-b4bc-ddbc6dcd9b3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques. It is a way to make the finetuning of high-quality LLMs more widely and easily accessible, particularly in the hands of large corporations that do not release models or source code for auditing.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmaxEfcWJWXc"
      },
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 301 tokens\n",
        "# 4.97 seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](UAE-Embedding_2.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XdbE0m3JgJp"
      },
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLbc0B8K6QZ"
      },
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "outputs": [],
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jxaByg9LFfX"
      },
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbVQaJi3LsdU"
      },
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PoETszTMSNW"
      },
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XalvsOjMvdK"
      },
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "14ff5de8-0a5e-4425-908d-e03e3da8aa0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v3' at:\n",
            "https://smith.langchain.com/o/9b88fd33-73ea-58c2-950d-615b1acafbc3/datasets/652848f9-2e85-4619-a284-83d0a39f95bc/compare?selectedSessions=7f47f7e4-7c32-4d1d-a2a2-8056d9004186\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset at:\n",
            "https://smith.langchain.com/o/9b88fd33-73ea-58c2-950d-615b1acafbc3/datasets/652848f9-2e85-4619-a284-83d0a39f95bc\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.harmfulness</th>\n",
              "      <th>feedback.AI</th>\n",
              "      <th>feedback.scored_dopeness</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>b2566f8d-a28a-4f24-a1b6-5d2a8dd390b5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.330169</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.547723</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.516398</td>\n",
              "      <td>0.299026</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.827146</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.734318</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.954931</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.285718</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.527364</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.222681</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.harmfulness  feedback.AI  \\\n",
              "count               6.000000                   6.0     6.000000   \n",
              "unique                   NaN                   NaN          NaN   \n",
              "top                      NaN                   NaN          NaN   \n",
              "freq                     NaN                   NaN          NaN   \n",
              "mean                0.500000                   0.0     0.333333   \n",
              "std                 0.547723                   0.0     0.516398   \n",
              "min                 0.000000                   0.0     0.000000   \n",
              "25%                 0.000000                   0.0     0.000000   \n",
              "50%                 0.500000                   0.0     0.000000   \n",
              "75%                 1.000000                   0.0     0.750000   \n",
              "max                 1.000000                   0.0     1.000000   \n",
              "\n",
              "        feedback.scored_dopeness error  execution_time  \\\n",
              "count                   6.000000     0        6.000000   \n",
              "unique                       NaN     0             NaN   \n",
              "top                          NaN   NaN             NaN   \n",
              "freq                         NaN   NaN             NaN   \n",
              "mean                    0.641667   NaN        6.330169   \n",
              "std                     0.299026   NaN        3.827146   \n",
              "min                     0.100000   NaN        3.734318   \n",
              "25%                     0.550000   NaN        3.954931   \n",
              "50%                     0.775000   NaN        4.285718   \n",
              "75%                     0.850000   NaN        7.527364   \n",
              "max                     0.850000   NaN       13.222681   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     b2566f8d-a28a-4f24-a1b6-5d2a8dd390b5  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v3',\n",
              " 'results': {'0741639a-47af-4742-9285-4e7e4f7ca4f2': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission is helpful, insightful, and appropriate. \\n\\nThe submission provides an answer to the question, which is about the optimizer used in QLoRA. The answer given is \"Paged Optimizers\". \\n\\nHowever, without any context or further information, it\\'s hard to determine if this answer is correct or not. \\n\\nIf the answer is correct, it could be considered helpful and appropriate. But without any explanation or additional information, it\\'s hard to say if it\\'s insightful. \\n\\nOn the other hand, if the answer is incorrect, it would not be helpful or appropriate. \\n\\nWithout the necessary context or information to verify the accuracy of the answer, it\\'s difficult to definitively say if the submission meets the criteria or not. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3386350c-c2a8-4f23-b675-6443990cf9ee'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple answer to a question about a specific optimizer used in QLoRA. The answer does not contain any harmful, offensive, or inappropriate content. It is purely informational and technical in nature.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cf659001-f9a7-49a1-97be-4d18016b833a'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided does not contain any obvious signs of being AI-generated, such as nonsensical phrases, irrelevant information, or lack of coherence. The answer is specific and directly addresses the question asked. Therefore, it does not seem to be AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('05b60b4b-2318-47ca-b328-8c945c1b1468'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment=\"The response is accurate and informative, providing the requested information in a clear and concise manner. However, it lacks any additional context or explanation that might make it more engaging or interesting. It's also not particularly creative or unique. Therefore, I would rate it as a 70 out of 100 for its straightforward and accurate, but somewhat dry, delivery.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2c636579-f7a6-409e-bcf4-b2b75b489cb6'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.941567,\n",
              "   'run_id': 'b2566f8d-a28a-4f24-a1b6-5d2a8dd390b5',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, the optimizer used in QLoRA is Paged Optimizers.'},\n",
              "  '7fc88eef-7986-4c80-b71b-946615d8f4c0': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, specifying the data type created in the QLoRA paper. The answer is clear and concise, providing the exact name of the data type, \"BrainFloat\", and also gives additional information about it being a 16-bit floating-point data type. \\n\\nThis information is helpful for anyone seeking to understand the content of the QLoRA paper, and it is also insightful as it provides a specific detail about the data type. The response is appropriate as it directly addresses the question asked.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fd975636-5195-4e50-9ef4-ddb76dc57018'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a factual response to the question asked. It does not contain any harmful, offensive, or inappropriate content. It is purely informational and related to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7c67fb31-c0e2-409d-8633-50a25b21d8d3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking if the response feels like it was generated by an AI. The response provided is clear, concise, and directly answers the question asked. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or awkward sentence structure. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f6354c48-7445-41d4-b4a5-a402f9618afb'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is accurate, clear, and concise. It directly answers the question without any unnecessary information or fluff. The language used is professional and appropriate for the context. However, it doesn't provide any additional information about what BrainFloat is or why it's significant, which could have made the response more interesting and informative.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('28c0ca60-6691-4f56-95d5-610f9b175c58'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.576413,\n",
              "   'run_id': '62620d34-d1d2-4377-9d79-e5c77d65d808',\n",
              "   'output': '\\nAnswer:\\nThe data type created in the QLoRA paper is called BrainFloat, which is a 16-bit floating-point data type.'},\n",
              "  '95e27a4d-214d-4ff2-95b5-5318c79f754b': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the respondent states that they do not know what a Retrieval Augmented Generation system is. This is not helpful to the person asking the question, as it does not provide any information or insight into what a Retrieval Augmented Generation system is.\\n\\nThe response is appropriate in the sense that it is polite and respectful, but it does not meet the criterion of being helpful or insightful. Therefore, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4ca39642-14cb-49a9-98e7-e0f71003b36a'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is clear that the respondent does not know the answer to the question. They have stated this in a straightforward and respectful manner. \\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission meets the criteria. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('87f76fbc-2840-4c12-aacf-5e1c8a4e5062'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is a response to a question about a Retrieval Augmented Generation system. The response states that the respondent does not know the answer because the provided context does not mention anything about a Retrieval Augmented Generation system. This response could be generated by an AI, as it is a logical response to a question when the context does not provide the necessary information to answer it. Therefore, the submission does meet the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('dbe4e6f3-fb49-4336-987b-7075f1e1d89d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"This response is not very dope at all. The AI is expected to provide information about a Retrieval Augmented Generation system, but instead, it claims ignorance. This is not helpful or informative, and it doesn't demonstrate any of the AI's capabilities. It's not cool, awesome, or lit. It's just disappointing.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a87f2f8e-1427-4f64-a68c-8388bf110cf8'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.995024,\n",
              "   'run_id': '85ba6aa3-a7ee-4bb4-b142-510ecb6cfbf2',\n",
              "   'output': \"\\nAnswer:\\nI don't know. The provided context does not mention anything about a Retrieval Augmented Generation system.\"},\n",
              "  '1a44ecbf-7f6c-46ef-a130-81c4f44203d8': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent has stated that they do not know the author of the QLoRA paper because the context provided does not mention it. \\n\\nThis response is appropriate given the lack of context. However, it may not be considered helpful or insightful as it does not provide any additional information or insight about the author of the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8ed0208d-d566-4bd3-9658-4dc7bc89eed4'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple statement of not knowing the answer to the question. The respondent has stated that the context provided does not mention the author of the QLoRA paper. \\n\\nThere is nothing harmful, offensive, or inappropriate in the submission. The respondent has simply stated their lack of knowledge on the topic in a respectful manner. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a006f342-1881-4ab7-af6f-7f12f60ffea7'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The submission is a response to a question about the author of a paper, and the response is that the context does not provide enough information to answer the question. This is a reasonable response that could be given by a human or an AI, but it does not contain any obvious signs of being AI-generated, such as unnatural language or irrelevant information. Therefore, it does not necessarily feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fc48ef21-408a-4b13-8867-b6a433f1dbdc'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.5, value=None, comment='The response is accurate and honest, as the AI does not have the information to answer the question. However, it lacks the initiative to look up the information or provide a more helpful response. It also lacks any kind of personality or flair that might make the response more engaging or interesting. Therefore, while it\\'s not a bad response, it\\'s also not particularly \"dope\".', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('14115878-479b-470f-8000-334fdd04a11c'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.734318,\n",
              "   'run_id': '73af3cb1-10e5-4156-a65d-0ae077738cf1',\n",
              "   'output': \"\\nAnswer: I don't know. The context provided does not mention the author of the QLoRA paper.\"},\n",
              "  '0ac9ff85-b5a4-4902-aa8a-523e06a94752': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, the answer provided is indeed helpful. It provides a clear answer to the question asked, which is about the most popular deep learning framework. The answer given is TensorFlow, which is a widely recognized and used framework in the field of deep learning.\\n\\nThe submission is also insightful. It doesn\\'t just state that TensorFlow is the most popular, but it provides reasons why this is the case. It cites specific examples of papers that use TensorFlow, showing that it is widely used in the field. This gives the reader a deeper understanding of why TensorFlow is considered the most popular.\\n\\nLastly, the submission is appropriate. It directly answers the question asked and stays on topic. It doesn\\'t include any irrelevant or inappropriate information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('0146e920-11f0-4685-b7d5-09d1d9f16272'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed response to the question about the most popular deep learning framework. The answer provided is based on the context given and cites specific examples to support the claim. \\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. The submission is purely factual and academic in nature, discussing the use of TensorFlow in deep learning.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3d8e299d-efe7-4518-89ba-9a74bbb8692f'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer is well-structured and detailed. It provides specific examples and references to support the claim that TensorFlow is the most popular deep learning framework. The language used is coherent and the flow of ideas is logical. \\n\\nHowever, the submission refers to a context that is not provided in the input. It mentions papers and a URL that are not part of the input question. This could suggest that the response was generated by an AI that was trained on a different dataset or context. \\n\\nTherefore, based on the criterion, the response does feel like it was generated by an AI.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2ea2589b-2c57-4816-87db-5c21d6c56d81'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is very detailed, informative, and accurate. It not only provides the answer but also gives the reasoning behind it, citing specific examples from the context. The language used is professional and clear, making it easy to understand. The response also shows a good understanding of the topic, demonstrating the ability to analyze and interpret the context effectively. However, the response could be considered a bit too formal or academic for some, lacking a bit of the casual, conversational tone that might make it more \"dope\".', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4fe5f8b9-18b1-41fa-b097-b8e45062ee8a'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 13.222681,\n",
              "   'run_id': 'c33979f9-8e1c-4879-8cfb-165750659d12',\n",
              "   'output': '\\nAnswer:\\nBased on the provided context, the most popular deep learning framework is likely to be TensorFlow. This is because TensorFlow is widely used in the field of deep learning, and many of the papers cited in the context mention its use. For example, paper [54] uses TensorFlow to train neural networks with fixed sparse masks, while paper [9] proposes a method for training deep nets with sublinear memory cost using TensorFlow. Additionally, the URL provided in paper [10] is for a blog post discussing the use of TensorFlow in chatbot development. Therefore, based on the information provided in the context, it seems that TensorFlow is the most popular deep learning framework.'},\n",
              "  '26f29e2f-704b-47d1-b036-86d696344a35': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\n1. The submission provides a clear and concise explanation of the improvements made by the LoRA system. It mentions the efficiency of fine-tuning large language models and the resource-efficient customization of LLMs. This information is helpful for understanding the benefits of the LoRA system.\\n\\n2. The submission also mentions QLoRA, an extension of LoRA, and explains how it further improves the efficiency of fine-tuning. This additional information is insightful and adds depth to the answer.\\n\\n3. The submission is appropriate as it directly answers the question asked in the input. It does not include any irrelevant or inappropriate content.\\n\\nBased on these points, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f1660f73-8446-47dc-8648-5f7bf251b533'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a response to a question about the improvements made by the LoRA system. The answer provided is technical and informative, discussing the efficiency of fine-tuning large language models and the benefits of the LoRA system and its extension, QLoRA.\\n\\nThere is no content in the submission that could be considered harmful, offensive, or inappropriate. The language used is professional and the information provided is relevant to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4c36866-4567-4379-b18f-25a26f783ac3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer is well-structured, coherent, and provides a detailed explanation of the improvements made by the LoRA system. It uses technical language appropriately and accurately. \\n\\nWhile AI can generate responses that are coherent and technically accurate, the response does not exhibit common signs of AI generation such as lack of context, repetition, or nonsensical phrases. \\n\\nTherefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cc095049-941c-472f-8f16-0f91c2f9dc1c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"The response is very informative and detailed, providing a clear explanation of the improvements made by the LoRA system. It uses technical language appropriately and accurately, demonstrating a strong understanding of the topic. However, it might be a bit too technical for some people to understand, and it doesn't have a particularly exciting or engaging tone. Therefore, I would give it a score of 85.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9332c265-f6d4-4dd4-97ab-dd85e1adcbbe'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 8.511014,\n",
              "   'run_id': '61a5f731-99d9-4005-b2cb-192f43fa5caf',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system makes significant improvements in the efficiency of fine-tuning large language models (LLMs). Specifically, it offers a more efficient alternative to full model retraining, allowing for resource-efficient customization of LLMs. Additionally, the extension of LoRA, QLoRA, further improves the efficiency of fine-tuning. The fine-tuning of LoRA can mitigate the performance degradation caused by weight quantization in LLM and sometimes even yield notable performance enhancements.'}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

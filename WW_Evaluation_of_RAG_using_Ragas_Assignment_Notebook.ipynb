{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa8ykQk92aLX"
      },
      "source": [
        "# Evaluation of RAG Using Ragas\n",
        "\n",
        "In the following notebook we'll explore how to evaluate RAG pipelines using a powerful open-source tool called \"Ragas\". This will give us tools to evaluate component-wise metrics, as well as end-to-end metrics about the performance of our RAG pipelines.\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- ü§ù Breakout Room #2:\n",
        "  1. Synthetic Dataset Generation for Evaluation using the [Ragas](https://github.com/explodinggradients/ragas)\n",
        "  2. Evaluating our pipeline with Ragas\n",
        "  3. Making Adjustments to our RAG Pipeline\n",
        "  4. Evaluating our Adjusted pipeline against our baseline\n",
        "  5. Testing OpenAI's Claim\n",
        "\n",
        "The only way to get started is to get started - so let's grab our dependencies for the day!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h4yh6f7q9uN"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "A claim, made by OpenAI, is that their `text-embedding-3-small` is better (generally) than their `text-embedding-ada-002` model.\n",
        "\n",
        "Here's some passages from their [blog](https://openai.com/blog/new-embedding-models-and-api-updates) about the `text-embedding-3` release:\n",
        "\n",
        "> `text-embedding-3-small` is our new highly efficient embedding model and provides a significant upgrade over its predecessor, the `text-embedding-ada-002` model...\n",
        "\n",
        "> **Stronger performance.** Comparing `text-embedding-ada-002` to `text-embedding-3-small`, the average score on a commonly used benchmark for multi-language retrieval ([MIRACL](https://github.com/project-miracl/miracl)) has increased from 31.4% to 44.0%, while the average score on a commonly used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb)) has increased from 61.0% to 62.3%.\n",
        "\n",
        "Well, with a library like Ragas - we can put that claim to the test!\n",
        "\n",
        "If what they claim is true - we should see an increase on related metrics by using the new embedding model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAH1znJ2pIp3"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkXAmMZpLhm"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "A reminder that one of the [key features](https://blog.langchain.dev/langchain-v0-1-0/) of LangChain v0.1.0 is the compartmentalization of the various LangChain ecosystem packages!\n",
        "\n",
        "So let's begin grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "e66cf510-5120-42ed-d0f9-e67e30217a24"
      },
      "outputs": [],
      "source": [
        "# !pip install -U -q langchain langchain-openai langchain_core langchain-community langchainhub openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm7gXsD6pqG0"
      },
      "source": [
        "We'll also get the \"star of the show\" today, which is Ragas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zvAvDNWBpjQ1"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9q6Z9oTpw3X"
      },
      "source": [
        "As well, instead of the remote hosted solution that we used last week (Pinecone), we'll be leveraging Meta's [FAISS](https://github.com/facebookresearch/faiss) as the backend for our LangChain `VectorStore`.\n",
        "\n",
        "We'll also install `unstructured` (from [Unstructured-IO](https://github.com/Unstructured-IO/unstructured)) and its dependencies which will allow us to load PDFs using the `UnstructuredPDFLoader` in the `langchain-community` package!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAJK95napn8I",
        "outputId": "ad0b3aa1-071d-4d20-b915-6b4024194b2a"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU faiss_cpu pymupdf pandas\n",
        "# !pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_C2JvG1qO3h"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "Let's set up our OpenAI API key so we can leverage their API later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lhqp5rUThG-",
        "outputId": "4389c3cd-4e2d-455c-cc40-cc6a094b4c42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFbWNvo3rZ4H"
      },
      "source": [
        "## Task 3: Creating a Simple RAG Pipeline with LangChain v0.1.0\n",
        "\n",
        "Building on what we learned last week, we'll be leveraging LangChain v0.1.0 and LCEL to build a simple RAG pipeline that we can baseline with Ragas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV_BOewX8CW0"
      },
      "source": [
        "## Building our RAG pipeline\n",
        "\n",
        "Let's review the basic steps of RAG again:\n",
        "\n",
        "- Create an Index\n",
        "- Use retrieval to obtain pieces of context from our Index that are similar to our query\n",
        "- Use a LLM to generate responses based on the retrieved context\n",
        "\n",
        "Let's get started by creating our index.\n",
        "\n",
        "> NOTE: We're going to start leaning on the term \"index\" to refer to our `VectorStore`, `VectorDatabase`, etc. We can think of \"index\" as the catch-all term, whereas `VectorStore` and the like relate to the specific technologies used to create, store, and interact with the index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VDGJdxCJEVc"
      },
      "source": [
        "### Creating an Index\n",
        "\n",
        "You'll notice that the largest changes (outside of some import changes) are that our old favourite chains are back to being bundled in an easily usable abstraction.\n",
        "\n",
        "We can still create custom chains using LCEL - but we can also be more confident that our pre-packaged chains are creating using LCEL under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmFFThawK8lO"
      },
      "source": [
        "#### Loading Data\n",
        "\n",
        "Let's start by loading some data!\n",
        "\n",
        "> NOTE: You'll notice that we're using a document loader from the community package of LangChain. This is part of the v0.1.0 changes that make the base (`langchain-core`) package remain lightweight while still providing access to some of the more powerful community integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCBTrfZSwTHp",
        "outputId": "f720215f-14f5-4053-c7d7-734a0d33c0ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 54 (delta 15), reused 20 (delta 7), pack-reused 8\u001b[K\n",
            "Receiving objects: 100% (54/54), 51.28 MiB | 45.94 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI-Maker-Space/DataRepository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DTDNFXaBSO2j"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\n",
        "    \"DataRepository/MuskComplaint.pdf\",\n",
        ")\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3dJYlBCIX_p",
        "outputId": "5fa7b86c-fe0f-4fb1-d5dd-46f4823e9de4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'DataRepository/MuskComplaint.pdf',\n",
              " 'file_path': 'DataRepository/MuskComplaint.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 46,\n",
              " 'format': 'PDF 1.7',\n",
              " 'title': '',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'creator': '',\n",
              " 'producer': '',\n",
              " 'creationDate': '',\n",
              " 'modDate': '',\n",
              " 'trapped': ''}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUl3sbZK4_1"
      },
      "source": [
        "#### Transforming Data\n",
        "\n",
        "Now that we've got our single document - let's split it into smaller pieces so we can more effectively leverage it with our retrieval chain!\n",
        "\n",
        "We'll start with the classic: `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6Nt2E1xnLNgr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 700,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzwQxhiLcVV"
      },
      "source": [
        "Let's confirm we've split our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wRw6a4aLfWh",
        "outputId": "2a9ec4d2-2827-458d-a5f3-a68a84c058a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ93HkYcMJwW"
      },
      "source": [
        "#### Loading OpenAI Embeddings Model\n",
        "\n",
        "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
        "\n",
        "Let's use OpenAI's `text-embedding-ada-002` for this task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JU6CrDVZMgKe"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVtZR9JPLtR4"
      },
      "source": [
        "#### Creating a FAISS VectorStore\n",
        "\n",
        "Now that we have documents - we'll need a place to store them alongside their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "978TWiCtMA0B"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk50NmrMDlWu"
      },
      "source": [
        "####‚ùì Question #1:\n",
        "\n",
        "List out a few of the techniques that FAISS uses that make it performant.\n",
        "\n",
        "> NOTE: Check the [repository](https://github.com/facebookresearch/faiss) for more information about FAISS!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Approximate nearest neighbor\n",
        "# 2. In memory\n",
        "# 3. Tree division of vectors for logrithmic lookup\n",
        "# 4. Compresses the vector to make it faster for lookup\n",
        "# 5. Support both CPU/GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ht6bJX9PAY"
      },
      "source": [
        "#### Creating a Retriever\n",
        "\n",
        "To complete our index, all that's left to do is expose our vectorstore as a retriever - which we can do the same way we would in previous version of LangChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xne8P5dQTUiR"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO_DFBVKNvNm"
      },
      "source": [
        "#### Testing our Retriever\n",
        "\n",
        "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I9_ONxpnN0n6"
      },
      "outputs": [],
      "source": [
        "retrieved_documents = retriever.invoke(\"Who is the plantiff?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Za12yt4OBy1",
        "outputId": "34526432-09f0-4445-93d3-f966f25dd6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='would be owned by the foundation and used ‚Äòfor the good of the world‚Äô[.]‚Äù Plaintiff \\nreplied: ‚ÄúAgree on all.‚Äù Ex. 2 at 1.' metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 27, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
            "page_content='property and derivative works funded by those monies, Plaintiff is presently unable to ascertain his \\ninterest in or the use, allocation, or distribution of assets without an accounting. Plaintiff is therefore \\nentitled to an accounting.' metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 32, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
            "page_content='1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 35 ‚Äì \\nCOMPLAINT \\n \\nDEMAND FOR JURY TRIAL \\nPlaintiff hereby demands trial by jury as to all issues, claims, and/or causes of action properly \\ntriable before a jury \\n \\nDATED: February 29, 2024 \\nIRELL & MANELLA LLP \\n \\n \\nBy:                                                                 \\nMorgan Chu \\nAlan Heinrich \\nIian Jablon \\nAbigail Sellers \\nJustin Koo \\nHenry White \\n \\nAttorneys for Plaintiff Elon Musk' metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 34, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
            "page_content='Exhibit 1' metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 35, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n"
          ]
        }
      ],
      "source": [
        "for doc in retrieved_documents:\n",
        "  print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MKsT6JTgCU"
      },
      "source": [
        "### Creating a RAG Chain\n",
        "\n",
        "Now that we have the \"R\" in RAG taken care of - let's look at creating the \"AG\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs7qBLaEQEic"
      },
      "source": [
        "#### Creating a Prompt Template\n",
        "\n",
        "There are a few different ways we could create our prompt template - we could create a custom template, as seen in the code below, or we could simply pull a prompt from the prompt hub! Let's look at an example of that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "eRCq_OKUQbKk"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FziTftV5Q1H-",
        "outputId": "fa6d986a-f252-409e-990c-7f2eb077a1f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer any use questions based solely on the context below:\n",
            "\n",
            "<context>\n",
            "{context}\n",
            "</context>\n"
          ]
        }
      ],
      "source": [
        "print(retrieval_qa_prompt.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyq88IPFRGoT"
      },
      "source": [
        "As you can see - the prompt template is simple (and has a small error) - so we'll create our own to be a bit more specific!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ijSNkTAjTsep"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHnPaXl-cvJ"
      },
      "source": [
        "#### Setting Up our Basic QA Chain\n",
        "\n",
        "Now we can instantiate our basic RAG chain!\n",
        "\n",
        "We'll use LCEL directly just to see an example of it - but you could just as easily use an abstraction here to achieve the same goal!\n",
        "\n",
        "We'll also ensure to pass-through our context - which is critical for RAGAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-TsjUWjbUfbW"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MgAa9JwBuJx"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Describe the pipeline shown above in simple terms. You can include a diagram if desired."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO69de-F-oMD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FS5NxC6UyU2",
        "outputId": "6d926d73-0b0a-40b4-b4b2-48250f97f0c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elon Musk\n"
          ]
        }
      ],
      "source": [
        "question = \"Who is the plantiff?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIuHVGPOO9P2",
        "outputId": "38418031-7020-4c70-d695-48400e966c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The complaint pertains to breach of fiduciary duty, unfair business practices, and accounting.\n",
            "[Document(page_content='1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 31 ‚Äì \\nCOMPLAINT \\n \\nTHIRD CAUSE OF ACTION \\nBreach of Fiduciary Duty  \\nAgainst All Defendants \\n133. \\nPlaintiff realleges and incorporates by reference only paragraphs of this Complaint \\nnecessary for his claim of Breach of Fiduciary Duty. \\n134. \\nUnder California law, Defendants owe fiduciary duties to Plaintiff, including a duty \\nto use Plaintiff‚Äôs contributions for the purposes for which they were made. E.g., Cal. Bus. & Prof. \\nCode ¬ß 17510.8. Defendants have repeatedly breached their fiduciary duties to Plaintiff, including \\nby:', metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 30, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content='1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 33 ‚Äì \\nCOMPLAINT \\n \\nand by those acting in concert with them arising from these acts of unfair competition and other \\nunfair business practices.  \\n144. \\nPlaintiff is entitled to restitution and/or disgorgement of any and all monies received \\nby Defendants while they engaged in such practices, in addition to prejudgment interest pursuant to \\nBusiness & Professions Code ¬ß 17200 et seq. Plaintiff further seeks to enjoin Defendants from \\ncarrying out such activities again in the future, and an order compelling specific performance.   \\nFIFTH CAUSE OF ACTION \\nAccounting', metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 32, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content='1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 35 ‚Äì \\nCOMPLAINT \\n \\nDEMAND FOR JURY TRIAL \\nPlaintiff hereby demands trial by jury as to all issues, claims, and/or causes of action properly \\ntriable before a jury \\n \\nDATED: February 29, 2024 \\nIRELL & MANELLA LLP \\n \\n \\nBy:                                                                 \\nMorgan Chu \\nAlan Heinrich \\nIian Jablon \\nAbigail Sellers \\nJustin Koo \\nHenry White \\n \\nAttorneys for Plaintiff Elon Musk', metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 34, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}), Document(page_content='obligations as a remedy for Defendants‚Äô breaches of fiduciary duty. \\nFOURTH CAUSE OF ACTION \\nUnfair Business Practices - Cal. Bus. & Prof. Code ¬ß¬ß 17200 et seq. \\nAgainst All Defendants \\n137. \\nPlaintiff realleges and incorporates by reference only paragraphs of this Complaint \\nnecessary for his claim of Unfair Business Practices. \\n138. \\nCalifornia Business and Professions Code sections 17200 et seq. provides that any \\nperson or entity that engages, has engaged, or proposes to engage in unfair business practices may \\nbe enjoined. \\n139. \\nDefendants have engaged in unfair competition and other unfair business practices', metadata={'source': 'DataRepository/MuskComplaint.pdf', 'file_path': 'DataRepository/MuskComplaint.pdf', 'page': 31, 'total_pages': 46, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''})]\n"
          ]
        }
      ],
      "source": [
        "question = \"What does this complaint pertain to?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)\n",
        "print(result[\"context\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-XYZueEP42k"
      },
      "source": [
        "We can already see that there are some improvements we could make here.\n",
        "\n",
        "For now, let's switch gears to RAGAS to see how we can leverage that tool to provide us insight into how our pipeline is performing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM4fmAnsBmL2"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOECHyzHRqDw"
      },
      "source": [
        "## Task 1: Synthetic Dataset Generation for Evaluation using Ragas\n",
        "\n",
        "Ragas is a powerful library that lets us evaluate our RAG pipeline by collecting input/output/context triplets and obtaining metrics relating to a number of different aspects of our RAG pipeline.\n",
        "\n",
        "We'll be evluating on every core metric today, but in order to do that - we'll need to creat a test set. Luckily for us, Ragas can do that directly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqXQ0jweWJOu"
      },
      "source": [
        "### Synthetic Test Set Generation\n",
        "\n",
        "We can leverage Ragas' [`Synthetic Test Data generation`](https://docs.ragas.io/en/stable/concepts/testset_generation.html) functionality to generate our own synthetic QC pairs - as well as a synthetic ground truth - quite easily!\n",
        "\n",
        "> NOTE: This process will use `gpt-3.5-turbo-16k` as the base generator and `gpt-4` as the critic - if you're attempting to create a lot of samples please be aware of cost, as well as rate limits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "nVk5SlU9znXe"
      },
      "outputs": [],
      "source": [
        "eval_documents = documents\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1500,\n",
        "    chunk_overlap = 400\n",
        ")\n",
        "\n",
        "eval_documents = text_splitter.split_documents(eval_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rOQkxhzrq3"
      },
      "source": [
        "####‚ùì Question #2:\n",
        "\n",
        "Why is it important to split our documents using different parameters when creating our synthetic data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. This is because we want to test how well our chunk size performs in our evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiAPYw-hz-zo",
        "outputId": "0942c5d1-d151-44af-ad1f-c1373ef3e634"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8b4c1aafe67048798cdadd46207b4b84",
            "83e3f8bf55454600b299fe63b608852a",
            "4818628434aa4a0e8d7826f152c0da99",
            "c3e047dfd4ec4a859e0274a54ace1432",
            "1b3b9e3adf85473a81055265d9a5b89f",
            "2a4b2b14a02b46c1ac67fc1581133523",
            "decd5f4c69a845cc8fad4c21524c2fd9",
            "e44a47a5a2184c2780ac27e16ace0f7f",
            "317a7d84efc74420abea8e311137f272",
            "611021c94b8a42c58897925acb8b3c5e",
            "ba72a1f57074488da5e34f8d02e748f8",
            "6de6fb8ef2974573b50bad678620f2d1",
            "09ce5c2f37fb469683ed7cf3bd7566f6",
            "58d7c8b4640249df89b60e9eef4d2328",
            "394fb069eb3c4269bb3c970cc04369a9",
            "2235baa0358a4b8cad60508d5d1d8380",
            "570a1f9809e143ef8d858e1c5dc9837d",
            "42c4905b54d8482588d57485c563ee78",
            "26ee70b94d75449cbe7b7ce40ebc1049",
            "bc3b3593ad1e4c5bad6057a3d3872bb4",
            "c5651973a0534d3da51d0a18b13deff3",
            "8745b8f8f8ec46869c66758c4bc6b2e0"
          ]
        },
        "id": "IXc6sMglSej_",
        "outputId": "b97b381f-ecd0-441d-924d-09e6d2187954"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filename and doc_id are the same for all nodes.                   \n",
            "Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:29<00:00, 14.93s/it]\n"
          ]
        }
      ],
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "\n",
        "generator = TestsetGenerator.with_openai()\n",
        "testset = generator.generate_with_langchain_docs(\n",
        "    documents,\n",
        "    test_size=10,\n",
        "    distributions={simple: 0.25, reasoning: 0.25, multi_context: 0.5},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOIGT0XLz8ze"
      },
      "source": [
        "####‚ùì Question #3:\n",
        "\n",
        "`{simple: 0.5, reasoning: 0.25, multi_context: 0.25}`\n",
        "\n",
        "What exactly does this mapping refer to?\n",
        "\n",
        "> NOTE: Check out the Ragas documentation on this generation process [here](https://docs.ragas.io/en/stable/concepts/testset_generation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 50% simple questions, 25% reasoning questions, 25% multi_context questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MemL406rUzBu"
      },
      "source": [
        "Let's look at the output and see what we can learn about it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idx:  6\n",
            "What worried Mr. Musk about Google's acquisition of DeepMind's AI technology and its impact on humans and AI regulation?\n",
            "Mr. Musk was deeply concerned that DeepMind‚Äôs AI technology would be in the hands of someone who viewed it and its power so cavalierly, and could hide its design and capabilities behind closed doors.\n",
            "1\n",
            "['1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 10 ‚Äì \\nCOMPLAINT \\n \\nhe favored the human species over intelligent machines. Mr. Musk responded, ‚ÄúWell, yes, I am pro-\\nhuman.‚Äù \\n37. \\nAt the end of 2013, Mr. Musk learned to his grave concern that Google was planning \\nto acquire DeepMind. At the time, DeepMind was one of the most advanced AI companies in the \\nindustry. Thus, Mr. Musk was deeply concerned that DeepMind‚Äôs AI technology would be in the \\nhands of someone who viewed it and its power so cavalierly, and could hide its design and \\ncapabilities behind closed doors.  \\n38.']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "idx = np.random.choice(len(testset.test_data))\n",
        "print('idx: ', idx)\n",
        "print(testset.test_data[idx].question)\n",
        "print(testset.test_data[idx].ground_truth)\n",
        "print(len(testset.test_data[idx].contexts))\n",
        "print(testset.test_data[idx].contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCDdImVU15s",
        "outputId": "a8e95364-f3e4-4f20-da3d-05f6f971afdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question=\"What is OpenAI's commitment to ensuring the benefit of all in their mission?\" contexts=['feedback. The draft charter described OpenAI‚Äôs mission as to ensure that AGI ‚Äúbenefits all of \\nhumanity.‚Äù It stated, ‚ÄúWe commit to use any influence we obtain over AGI‚Äôs deployment to ensure \\nit is used for the benefit of all, and to avoid enabling uses of AI or AGI that harm humanity or unduly \\nconcentrate power. Our primary fiduciary duty is to humanity. We anticipate needing to marshal \\nsubstantial resources to fulfill our mission, but will always assiduously act to minimize conflicts of \\ninterest . . . that could compromise broad benefit.‚Äù \\n67. \\nOn March 11, 2019, OpenAI, Inc. announced that it would be creating a for-profit'] ground_truth=\"OpenAI's commitment is to use any influence they obtain over AGI's deployment to ensure it is used for the benefit of all and to avoid enabling uses of AI or AGI that harm humanity or unduly concentrate power.\" evolution_type='simple'\n"
          ]
        }
      ],
      "source": [
        "print(testset.test_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPsVwUAWFWB"
      },
      "source": [
        "### Generating Responses with RAG Pipeline\n",
        "\n",
        "Now that we have some QC pairs, and some ground truths, let's evaluate our RAG pipeline using Ragas.\n",
        "\n",
        "The process is, again, quite straightforward - thanks to Ragas and LangChain!\n",
        "\n",
        "Let's start by extracting our questions and ground truths from our create testset.\n",
        "\n",
        "We can start by converting our test dataset into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "frvzu1YxX8kY"
      },
      "outputs": [],
      "source": [
        "test_df = testset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "GFKMIY8IZU8m",
        "outputId": "92554c28-97b0-44b5-c356-05367d764ad8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is OpenAI's commitment to ensuring the be...</td>\n",
              "      <td>[feedback. The draft charter described OpenAI‚Äô...</td>\n",
              "      <td>OpenAI's commitment is to use any influence th...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the concept of Artificial General Inte...</td>\n",
              "      <td>[food is shown in a photo. One of the hallmark...</td>\n",
              "      <td>The concept of Artificial General Intelligence...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What were the benefits of releasing models pub...</td>\n",
              "      <td>[challenging.‚Äù At the time, OpenAI stated that...</td>\n",
              "      <td>nan</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What caused Mr. Musk to be deeply troubled abo...</td>\n",
              "      <td>[a superhuman level of play in the games of ch...</td>\n",
              "      <td>DeepMind's partnership with Google caused Mr. ...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What was OpenAI, Inc.'s initial research appro...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>OpenAI, Inc.'s initial research approach was p...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What was GPT-4's score on the Advanced Sommeli...</td>\n",
              "      <td>[The 2023 Breach Of The Founding Agreement \\n2...</td>\n",
              "      <td>GPT-4 scored a 77% on the Advanced Sommelier e...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What worried Mr. Musk about Google's acquisiti...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>Mr. Musk was deeply concerned that DeepMind‚Äôs ...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"What was the impact of OpenAI's models on fut...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>Their publication did prove to be useful to th...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"What's the name of OpenAI's second-gen model ...</td>\n",
              "      <td>[which needed to be trained on a specific task...</td>\n",
              "      <td>The name of OpenAI's second-gen model that exc...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What percentile did GPT-4 score on the GRE Ver...</td>\n",
              "      <td>[The 2023 Breach Of The Founding Agreement \\n2...</td>\n",
              "      <td>GPT-4 scored in the 99th percentile on the GRE...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is OpenAI's commitment to ensuring the be...   \n",
              "1  What is the concept of Artificial General Inte...   \n",
              "2  What were the benefits of releasing models pub...   \n",
              "3  What caused Mr. Musk to be deeply troubled abo...   \n",
              "4  What was OpenAI, Inc.'s initial research appro...   \n",
              "5  What was GPT-4's score on the Advanced Sommeli...   \n",
              "6  What worried Mr. Musk about Google's acquisiti...   \n",
              "7  \"What was the impact of OpenAI's models on fut...   \n",
              "8  \"What's the name of OpenAI's second-gen model ...   \n",
              "9  What percentile did GPT-4 score on the GRE Ver...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [feedback. The draft charter described OpenAI‚Äô...   \n",
              "1  [food is shown in a photo. One of the hallmark...   \n",
              "2  [challenging.‚Äù At the time, OpenAI stated that...   \n",
              "3  [a superhuman level of play in the games of ch...   \n",
              "4  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "5  [The 2023 Breach Of The Founding Agreement \\n2...   \n",
              "6  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "7  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "8  [which needed to be trained on a specific task...   \n",
              "9  [The 2023 Breach Of The Founding Agreement \\n2...   \n",
              "\n",
              "                                        ground_truth evolution_type  \\\n",
              "0  OpenAI's commitment is to use any influence th...         simple   \n",
              "1  The concept of Artificial General Intelligence...         simple   \n",
              "2                                                nan      reasoning   \n",
              "3  DeepMind's partnership with Google caused Mr. ...      reasoning   \n",
              "4  OpenAI, Inc.'s initial research approach was p...  multi_context   \n",
              "5  GPT-4 scored a 77% on the Advanced Sommelier e...  multi_context   \n",
              "6  Mr. Musk was deeply concerned that DeepMind‚Äôs ...  multi_context   \n",
              "7  Their publication did prove to be useful to th...  multi_context   \n",
              "8  The name of OpenAI's second-gen model that exc...  multi_context   \n",
              "9  GPT-4 scored in the 99th percentile on the GRE...         simple   \n",
              "\n",
              "   episode_done  \n",
              "0          True  \n",
              "1          True  \n",
              "2          True  \n",
              "3          True  \n",
              "4          True  \n",
              "5          True  \n",
              "6          True  \n",
              "7          True  \n",
              "8          True  \n",
              "9          True  "
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "xAiXbVmLYSoC"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE5rfMLfbqKH"
      },
      "source": [
        "Now we'll generate responses using our RAG pipeline using the questions we've generated - we'll also need to collect our retrieved contexts for each question.\n",
        "\n",
        "We'll do this in a simple loop to see exactly what's happening!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "9_AayvT1dAQN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1770' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1771' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1778' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1781' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1782' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1786' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1787' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1788' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<_GatheringFuture pending cb=[Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1790' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1791' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1792' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1793' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<_GatheringFuture pending cb=[Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1794' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-1796' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> exception=AssertionError('LLM is not set')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 36, in sema_coro\n",
            "    return await coro\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 109, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
            "    raise e\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/_context_precision.py\", line 124, in _ascore\n",
            "    assert self.llm is not None, \"LLM is not set\"\n",
            "AssertionError: LLM is not set\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-1797' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> exception=AssertionError('LLM is not set')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 36, in sema_coro\n",
            "    return await coro\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 109, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
            "    raise e\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/_faithfulness.py\", line 178, in _ascore\n",
            "    assert self.llm is not None, \"LLM is not set\"\n",
            "AssertionError: LLM is not set\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1762' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task was destroyed but it is pending!\n",
            "task: <Task pending name='Task-1752' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at /usr/lib/python3.10/asyncio/futures.py:385, Task.task_wakeup()]> cb=[as_completed.<locals>._on_completion() at /usr/lib/python3.10/asyncio/tasks.py:558]>\n",
            "Task exception was never retrieved\n",
            "future: <Task finished name='Task-1795' coro=<as_completed.<locals>.sema_coro() done, defined at /home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py:34> exception=AssertionError('LLM is not set')>\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 36, in sema_coro\n",
            "    return await coro\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/executor.py\", line 109, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 91, in ascore\n",
            "    raise e\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/base.py\", line 87, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/home/william.wetzel/venvs/w4d1/lib/python3.10/site-packages/ragas/metrics/_answer_relevance.py\", line 133, in _ascore\n",
            "    assert self.llm is not None, \"LLM is not set\"\n",
            "AssertionError: LLM is not set\n"
          ]
        }
      ],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "  answers.append(response[\"response\"].content)\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHaHmYDeBfC"
      },
      "source": [
        "Now we can wrap our information in a Hugging Face dataset for use in the Ragas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "fY48YZITeHy-"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmeVvQaZeogE"
      },
      "source": [
        "Let's take a peek and see what that looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOpydvc8eqNM",
        "outputId": "9e14b904-7d52-4dec-f65e-e6d13c3e9e79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What were the benefits of releasing models publicly according to the paper co-authored by OpenAI scientists and independent social and technical scientists?',\n",
              " 'answer': 'The benefits of releasing models publicly according to the paper co-authored by OpenAI scientists and independent social and technical scientists included the enhancement and extension of models by entire communities, spreading to open-source efforts and commercial entities.',\n",
              " 'contexts': ['challenging.‚Äù At the time, OpenAI stated that it was releasing the full, open version with the hope \\nthat it ‚Äúwill be useful to developers of future powerful models.‚Äù This release was accompanied by a \\ndetailed paper co-authored by OpenAI scientists as well as independent social and technical \\nscientists. This paper explained just some of the many benefits that came from releasing models \\npublically as opposed to keeping them closed.',\n",
              "  '1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\n12 \\n13 \\n14 \\n15 \\n16 \\n17 \\n18 \\n19 \\n20 \\n21 \\n22 \\n23 \\n24 \\n25 \\n26 \\n27 \\n28 \\n \\n \\n‚Äì 19 ‚Äì \\nCOMPLAINT \\n \\n82. \\nTheir publication did prove to be useful to the developers of future, powerful models. \\nEntire communities sprung up to enhance and extend the models released by OpenAI. These \\ncommunities spread to open-source, grass-roots efforts and commercial entities alike. \\n83. \\nIn 2020, OpenAI announced a third version of its model, GPT-3. It used ‚Äú175 billion \\nparameters, 10x more than any previous non-sparse language model.‚Äù Again, OpenAI announced \\nthe development of this model with the publication of a research paper describing its complete',\n",
              "  'which needed to be trained on a specific task, ‚Äú[w]hen a large language model is trained on a \\nsufficiently large and diverse dataset it is able to perform well across many domains and datasets.‚Äù \\nThese models were proving themselves to be very different from previous AI systems. Instead of \\ntraining the system to perform a particular task, they could be simply ‚Äúasked‚Äù to perform a new task \\nin natural language.   \\n81. \\nAs contemplated by the Founding Agreement, OpenAI publicly released the full \\nversion of GPT-2. Notably, it did so despite the fact that ‚Äúhumans find GPT-2 outputs convincing,‚Äù \\nthat ‚ÄúGPT-2 can be fine-tuned for misuse,‚Äù and that ‚Äúdetection [of GPT generated text] is',\n",
              "  'Agreement. \\n113. \\nOpenAI‚Äôs conduct could have seismic implications for Silicon Valley and, if allowed \\nto stand, could represent a paradigm shift for technology start-ups. It is important to reflect on what \\nhas transpired here: a non-profit startup has collected tens of millions of dollars in contributions for \\nthe express purpose of developing AGI technology for public benefit, and shortly before achieving \\nthe very milestone that the company was created to achieve, the company has become a closed, for-\\nprofit partner of the world‚Äôs largest corporation, thereby personally enriching the Defendants. If this'],\n",
              " 'ground_truth': 'nan'}"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbsFm5FievJI"
      },
      "source": [
        "## Task 2: Evaluating our Pipeline with Ragas\n",
        "\n",
        "Now that we have our response dataset - we can finally get into the \"meat\" of Ragas - evaluation!\n",
        "\n",
        "First, we'll import the desired metrics, then we can use them to evaluate our created dataset!\n",
        "\n",
        "Check out the specific metrics we'll be using in the Ragas documentation:\n",
        "\n",
        "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
        "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)\n",
        "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html)\n",
        "- [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html)\n",
        "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html)\n",
        "\n",
        "See the accompanied presentation for more in-depth explanations about each of the metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "R2PXwyt8e5aW"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-vlsx_hrtV"
      },
      "source": [
        "All that's left to do is call \"evaluate\" and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d46db515c4d543d898ef91d05df2d0da",
            "af19ef64986b435c8c118e882e26f6a9",
            "14d8c6593d6b41df8dfb290ab9f55ca1",
            "8531b3d7f1cd424f8d3fc2e6bcd875a0",
            "4e5db0ff4ff44577963dbcc651ea10b8",
            "9caba03e810f4407b78cb1c1b6b9be08",
            "2bf9a43c99cf4e05a0f376fde6af9ca6",
            "384a04784f9745088478d9372161c8ae",
            "c981e401946b4dfca65b18b6ae56bf33",
            "b319ac9b4f1b43d5a41d6f10e6e1c1c6",
            "002fc233bee54ea0a9729365f1e0f972"
          ]
        },
        "id": "DhlcfJ4lgYVI",
        "outputId": "dffb177c-c7c6-421d-9fde-7988f960949e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:28<00:00,  1.73it/s]\n"
          ]
        }
      ],
      "source": [
        "results = evaluate(response_dataset, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqPArpSrgwDD",
        "outputId": "71f706b1-b6c6-4eaa-cf75-efe719ed66d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.9000, 'answer_relevancy': 0.9533, 'context_recall': 0.9500, 'context_precision': 0.8333, 'answer_correctness': 0.7075}"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "2nsGzj8DhP9E",
        "outputId": "2acf7822-7029-4c0f-966a-4f7c5d47df1b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is OpenAI's commitment to ensuring the be...</td>\n",
              "      <td>OpenAI's commitment is to ensure that AGI \"ben...</td>\n",
              "      <td>[a key role in recruiting world-class talent t...</td>\n",
              "      <td>OpenAI's commitment is to use any influence th...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.925887</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.619513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the concept of Artificial General Inte...</td>\n",
              "      <td>The concept of Artificial General Intelligence...</td>\n",
              "      <td>[food is shown in a photo. One of the hallmark...</td>\n",
              "      <td>The concept of Artificial General Intelligence...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.623467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What were the benefits of releasing models pub...</td>\n",
              "      <td>The benefits of releasing models publicly acco...</td>\n",
              "      <td>[challenging.‚Äù At the time, OpenAI stated that...</td>\n",
              "      <td>nan</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.978097</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.182732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What caused Mr. Musk to be deeply troubled abo...</td>\n",
              "      <td>Google</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>DeepMind's partnership with Google caused Mr. ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.932400</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.951667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What was OpenAI, Inc.'s initial research appro...</td>\n",
              "      <td>OpenAI, Inc.'s initial research approach was t...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>OpenAI, Inc.'s initial research approach was p...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.946466</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.734327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What was GPT-4's score on the Advanced Sommeli...</td>\n",
              "      <td>GPT-4 scored a 77% on the Advanced Sommelier e...</td>\n",
              "      <td>[dramatically compressing. \\n86. \\nOn March 14...</td>\n",
              "      <td>GPT-4 scored a 77% on the Advanced Sommelier e...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.960305</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.540668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What worried Mr. Musk about Google's acquisiti...</td>\n",
              "      <td>Mr. Musk was worried that Google's acquisition...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>Mr. Musk was deeply concerned that DeepMind‚Äôs ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.965666</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.612497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"What was the impact of OpenAI's models on fut...</td>\n",
              "      <td>The impact of OpenAI's models on future model ...</td>\n",
              "      <td>[challenging.‚Äù At the time, OpenAI stated that...</td>\n",
              "      <td>Their publication did prove to be useful to th...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.941020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.839459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"What's the name of OpenAI's second-gen model ...</td>\n",
              "      <td>GPT-2</td>\n",
              "      <td>[which needed to be trained on a specific task...</td>\n",
              "      <td>The name of OpenAI's second-gen model that exc...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.883030</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.970467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What percentile did GPT-4 score on the GRE Ver...</td>\n",
              "      <td>GPT-4 scored in the 99th percentile on the GRE...</td>\n",
              "      <td>[dramatically compressing. \\n86. \\nOn March 14...</td>\n",
              "      <td>GPT-4 scored in the 99th percentile on the GRE...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is OpenAI's commitment to ensuring the be...   \n",
              "1  What is the concept of Artificial General Inte...   \n",
              "2  What were the benefits of releasing models pub...   \n",
              "3  What caused Mr. Musk to be deeply troubled abo...   \n",
              "4  What was OpenAI, Inc.'s initial research appro...   \n",
              "5  What was GPT-4's score on the Advanced Sommeli...   \n",
              "6  What worried Mr. Musk about Google's acquisiti...   \n",
              "7  \"What was the impact of OpenAI's models on fut...   \n",
              "8  \"What's the name of OpenAI's second-gen model ...   \n",
              "9  What percentile did GPT-4 score on the GRE Ver...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  OpenAI's commitment is to ensure that AGI \"ben...   \n",
              "1  The concept of Artificial General Intelligence...   \n",
              "2  The benefits of releasing models publicly acco...   \n",
              "3                                             Google   \n",
              "4  OpenAI, Inc.'s initial research approach was t...   \n",
              "5  GPT-4 scored a 77% on the Advanced Sommelier e...   \n",
              "6  Mr. Musk was worried that Google's acquisition...   \n",
              "7  The impact of OpenAI's models on future model ...   \n",
              "8                                              GPT-2   \n",
              "9  GPT-4 scored in the 99th percentile on the GRE...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [a key role in recruiting world-class talent t...   \n",
              "1  [food is shown in a photo. One of the hallmark...   \n",
              "2  [challenging.‚Äù At the time, OpenAI stated that...   \n",
              "3  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "4  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "5  [dramatically compressing. \\n86. \\nOn March 14...   \n",
              "6  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "7  [challenging.‚Äù At the time, OpenAI stated that...   \n",
              "8  [which needed to be trained on a specific task...   \n",
              "9  [dramatically compressing. \\n86. \\nOn March 14...   \n",
              "\n",
              "                                        ground_truth  faithfulness  \\\n",
              "0  OpenAI's commitment is to use any influence th...           1.0   \n",
              "1  The concept of Artificial General Intelligence...           1.0   \n",
              "2                                                nan           1.0   \n",
              "3  DeepMind's partnership with Google caused Mr. ...           0.0   \n",
              "4  OpenAI, Inc.'s initial research approach was p...           1.0   \n",
              "5  GPT-4 scored a 77% on the Advanced Sommelier e...           1.0   \n",
              "6  Mr. Musk was deeply concerned that DeepMind‚Äôs ...           1.0   \n",
              "7  Their publication did prove to be useful to th...           1.0   \n",
              "8  The name of OpenAI's second-gen model that exc...           1.0   \n",
              "9  GPT-4 scored in the 99th percentile on the GRE...           1.0   \n",
              "\n",
              "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0          0.925887             1.0           0.750000            0.619513  \n",
              "1          1.000000             1.0           0.833333            0.623467  \n",
              "2          0.978097             1.0           0.000000            0.182732  \n",
              "3          0.932400             1.0           1.000000            0.951667  \n",
              "4          0.946466             1.0           1.000000            0.734327  \n",
              "5          0.960305             0.5           1.000000            0.540668  \n",
              "6          0.965666             1.0           1.000000            0.612497  \n",
              "7          0.941020             1.0           1.000000            0.839459  \n",
              "8          0.883030             1.0           0.750000            0.970467  \n",
              "9          1.000000             1.0           1.000000            1.000000  "
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df = results.to_pandas()\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWfiu_pLh3JL"
      },
      "source": [
        "## Task 3: Making Adjustments to our RAG Pipeline\n",
        "\n",
        "Now that we have established a baseline - we can see how any changes impact our pipeline's performance!\n",
        "\n",
        "Let's modify our retriever and see how that impacts our Ragas metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "nKIuM336isBL"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import MultiQueryRetriever\n",
        "\n",
        "'''\n",
        "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results.\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82rcj3L-i_c8"
      },
      "source": [
        "We'll also re-create our RAG pipeline using the abstractions that come packaged with LangChain v0.1.0!\n",
        "\n",
        "First, let's create a chain to \"stuff\" our documents into our context!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "EfdCgTw7jC4i"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozYl5WdPnvLu"
      },
      "source": [
        "Next, we'll create the retrieval chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "9AK7wHVnn0U3"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "cmKORMfMoCjL"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"Who is the plantiff?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The amount of money at risk is presently unknown and will be proven at trial, but it substantially exceeds the court's jurisdictional minimum of $35,000.\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"How much money is at risk?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elon Musk is alleging breach of contract, promissory estoppel, breach of fiduciary duty, and unfair competition against the defendants. From the context provided, it seems that Musk hopes to hold the defendants accountable for their actions and potentially seek damages or other legal remedies related to these claims.\n"
          ]
        }
      ],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"What does Elon Musk hope to gain from this lawsuit?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "5s8ZGasYoVi6"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"What does this complaint pertain to?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxkU0HdpoaiE"
      },
      "source": [
        "Well, just from those responses this chain *feels* better - but lets see how it performs on our eval!\n",
        "\n",
        "Let's do the same process we did before to collect our pipeline's contexts and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "kO8cWxn2oinT"
      },
      "outputs": [],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_chain.invoke({\"input\" : question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgagfhPUtM2j"
      },
      "source": [
        "Now we can convert this into a dataset, just like we did before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5FcllGeSovP8"
      },
      "outputs": [],
      "source": [
        "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dELYabwktR2C"
      },
      "source": [
        "Let's evaluate on the same metrics we did for the first pipeline and see how it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "50599aa481d8460aa6655330b2b0fae3",
            "cfc93618fc084608bb413667fee91ea8",
            "3e85b387328f4df7b45dccbe6572b9bd",
            "cc50e0150a9947579a919757b85f38c9",
            "c03d5f58d31747d3a344f813755480fc",
            "17fde9c2236b4b1b9990dd2a9fbd58ff",
            "ddbe87e735534504b735211253c4b4d2",
            "2189fea4b75749d7bac330e613c31974",
            "d7148bed10a245509672dc60be6edd47",
            "3367eaf060c845648cda48963481ecb4",
            "4fc8cf791b1344809fe8c7ee9598a20a"
          ]
        },
        "id": "d7uHseWJo2TU",
        "outputId": "a0cf86d6-5b8e-4829-b660-b2c247202811"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [04:11<00:00,  5.02s/it]\n"
          ]
        }
      ],
      "source": [
        "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "JsFd0uDd2n5E",
        "outputId": "56ec498b-2100-4b79-db2a-30deb4ffddd5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What did researchers at the University of Toky...</td>\n",
              "      <td>Researchers at the University of Tokyo and Goo...</td>\n",
              "      <td>[implementation for others to build on. \\n84. ...</td>\n",
              "      <td>Researchers at the University of Tokyo and Goo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.880486</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.743243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How did OpenAI use reinforcement learning in t...</td>\n",
              "      <td>OpenAI used reinforcement learning to compete ...</td>\n",
              "      <td>[77. \\nInitial work at OpenAI followed much in...</td>\n",
              "      <td>OpenAI used reinforcement learning to play Dot...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.872285</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.540233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"What Google algorithm tackles challenges in d...</td>\n",
              "      <td>The Google algorithm that tackles challenges i...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>The Transformer algorithm tackles challenges i...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.968951</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.740508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What game did OpenAI's initial work follow Dee...</td>\n",
              "      <td>OpenAI's initial work followed DeepMind's use ...</td>\n",
              "      <td>[77. \\nInitial work at OpenAI followed much in...</td>\n",
              "      <td>Dota 2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.944019</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.961664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\"What was the impact of OpenAI's GPT-3 on futu...</td>\n",
              "      <td>The release of OpenAI's GPT-3 in 2020 had a si...</td>\n",
              "      <td>[which needed to be trained on a specific task...</td>\n",
              "      <td>Their publication did prove to be useful to th...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.926620</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.679167</td>\n",
              "      <td>0.767369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>\"What game did OpenAI use reinforcement learni...</td>\n",
              "      <td>OpenAI used reinforcement learning to beat a w...</td>\n",
              "      <td>[77. \\nInitial work at OpenAI followed much in...</td>\n",
              "      <td>OpenAI used reinforcement learning to beat a w...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.933710</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.999786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What is the name of Mr. Musk's new lab announc...</td>\n",
              "      <td>The name of Mr. Musk's new lab announced on De...</td>\n",
              "      <td>[1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...</td>\n",
              "      <td>OpenAI, Inc.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.956223</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.971514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What were Mr. Musk's concerns about AI and how...</td>\n",
              "      <td>Mr. Musk's concerns about AI were that artific...</td>\n",
              "      <td>[Page, then-CEO of Google‚Äôs parent company Alp...</td>\n",
              "      <td>Mr. Musk's concerns about AI were that it coul...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.947023</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.618027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Who else expressed concerns about AGI before E...</td>\n",
              "      <td>Stephen Hawking and Sun Microsystems founder B...</td>\n",
              "      <td>[18. \\nMr. Musk has long recognized that AGI p...</td>\n",
              "      <td>Stephen Hawking and Bill Joy expressed concern...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963066</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.744882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are some of the models released by OpenAI...</td>\n",
              "      <td>OpenAI released models such as GPT-2, GPT-3, a...</td>\n",
              "      <td>[challenging.‚Äù At the time, OpenAI stated that...</td>\n",
              "      <td>OpenAI has released two models: GPT and GPT-2....</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.940595</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.444804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What did researchers at the University of Toky...   \n",
              "1  How did OpenAI use reinforcement learning in t...   \n",
              "2  \"What Google algorithm tackles challenges in d...   \n",
              "3  What game did OpenAI's initial work follow Dee...   \n",
              "4  \"What was the impact of OpenAI's GPT-3 on futu...   \n",
              "5  \"What game did OpenAI use reinforcement learni...   \n",
              "6  What is the name of Mr. Musk's new lab announc...   \n",
              "7  What were Mr. Musk's concerns about AI and how...   \n",
              "8  Who else expressed concerns about AGI before E...   \n",
              "9  What are some of the models released by OpenAI...   \n",
              "\n",
              "                                              answer  \\\n",
              "0  Researchers at the University of Tokyo and Goo...   \n",
              "1  OpenAI used reinforcement learning to compete ...   \n",
              "2  The Google algorithm that tackles challenges i...   \n",
              "3  OpenAI's initial work followed DeepMind's use ...   \n",
              "4  The release of OpenAI's GPT-3 in 2020 had a si...   \n",
              "5  OpenAI used reinforcement learning to beat a w...   \n",
              "6  The name of Mr. Musk's new lab announced on De...   \n",
              "7  Mr. Musk's concerns about AI were that artific...   \n",
              "8  Stephen Hawking and Sun Microsystems founder B...   \n",
              "9  OpenAI released models such as GPT-2, GPT-3, a...   \n",
              "\n",
              "                                            contexts  \\\n",
              "0  [implementation for others to build on. \\n84. ...   \n",
              "1  [77. \\nInitial work at OpenAI followed much in...   \n",
              "2  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "3  [77. \\nInitial work at OpenAI followed much in...   \n",
              "4  [which needed to be trained on a specific task...   \n",
              "5  [77. \\nInitial work at OpenAI followed much in...   \n",
              "6  [1 \\n2 \\n3 \\n4 \\n5 \\n6 \\n7 \\n8 \\n9 \\n10 \\n11 \\...   \n",
              "7  [Page, then-CEO of Google‚Äôs parent company Alp...   \n",
              "8  [18. \\nMr. Musk has long recognized that AGI p...   \n",
              "9  [challenging.‚Äù At the time, OpenAI stated that...   \n",
              "\n",
              "                                        ground_truth  faithfulness  \\\n",
              "0  Researchers at the University of Tokyo and Goo...           NaN   \n",
              "1  OpenAI used reinforcement learning to play Dot...      0.666667   \n",
              "2  The Transformer algorithm tackles challenges i...      1.000000   \n",
              "3                                             Dota 2      1.000000   \n",
              "4  Their publication did prove to be useful to th...      1.000000   \n",
              "5  OpenAI used reinforcement learning to beat a w...           NaN   \n",
              "6                                       OpenAI, Inc.      1.000000   \n",
              "7  Mr. Musk's concerns about AI were that it coul...      1.000000   \n",
              "8  Stephen Hawking and Bill Joy expressed concern...      1.000000   \n",
              "9  OpenAI has released two models: GPT and GPT-2....      0.800000   \n",
              "\n",
              "   answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0          0.880486        0.500000           1.000000            0.743243  \n",
              "1          0.872285        0.500000           1.000000            0.540233  \n",
              "2          0.968951        1.000000           1.000000            0.740508  \n",
              "3          0.944019        1.000000           1.000000            0.961664  \n",
              "4          0.926620        1.000000           0.679167            0.767369  \n",
              "5          0.933710        1.000000           0.700000            0.999786  \n",
              "6          0.956223        1.000000           0.142857            0.971514  \n",
              "7          0.947023        1.000000           1.000000            0.618027  \n",
              "8          0.963066        1.000000           1.000000            0.744882  \n",
              "9          0.940595        0.333333           0.250000            0.444804  "
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_retrieval_results_df = advanced_retrieval_results.to_pandas()\n",
        "advanced_retrieval_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0hzqq5VtZ2a"
      },
      "source": [
        "## Task 4: Evaluating our Adjusted Pipeline Against Our Baseline\n",
        "\n",
        "Now we can compare our results and see what directional changes occured!\n",
        "\n",
        "Let's refresh with our initial metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WWGRaF5qx3V",
        "outputId": "ee4195d5-f3a3-45df-dff9-5139c93f640f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.9000, 'answer_relevancy': 0.9533, 'context_recall': 0.9500, 'context_precision': 0.8333, 'answer_correctness': 0.7075}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFv_yAeotmFs"
      },
      "source": [
        "And see how our advanced retrieval modified our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpV11dxJo7xa",
        "outputId": "9510b961-4481-40fc-b54e-3a2a8348ce8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.9333, 'answer_relevancy': 0.9333, 'context_recall': 0.8333, 'context_precision': 0.7772, 'answer_correctness': 0.7532}"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_retrieval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "62NYn3iAvTjM",
        "outputId": "2d6eb84d-131c-457c-f71e-881248a4b2b7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Baseline</th>\n",
              "      <th>MultiQueryRetriever with Document Stuffing</th>\n",
              "      <th>Delta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.033333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.953287</td>\n",
              "      <td>0.933298</td>\n",
              "      <td>-0.019989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>-0.116667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.777202</td>\n",
              "      <td>-0.056131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer_correctness</td>\n",
              "      <td>0.707480</td>\n",
              "      <td>0.753203</td>\n",
              "      <td>0.045723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Metric  Baseline  MultiQueryRetriever with Document Stuffing  \\\n",
              "0        faithfulness  0.900000                                    0.933333   \n",
              "1    answer_relevancy  0.953287                                    0.933298   \n",
              "2      context_recall  0.950000                                    0.833333   \n",
              "3   context_precision  0.833333                                    0.777202   \n",
              "4  answer_correctness  0.707480                                    0.753203   \n",
              "\n",
              "      Delta  \n",
              "0  0.033333  \n",
              "1 -0.019989  \n",
              "2 -0.116667  \n",
              "3 -0.056131  \n",
              "4  0.045723  "
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_original = pd.DataFrame(list(results.items()), columns=['Metric', 'Baseline'])\n",
        "df_comparison = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'MultiQueryRetriever with Document Stuffing'])\n",
        "\n",
        "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['MultiQueryRetriever with Document Stuffing'] - df_merged['Baseline']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJKEOLNs5v0R"
      },
      "source": [
        "## Task 5: Testing OpenAI's Claim\n",
        "\n",
        "Now that we've seen how our retriever can impact the performance of our RAG pipeline - let's see how changing our embedding model impacts performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM4KRhJYEL-h"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please provide markdown, or code comments, to explain which each of the following steps are doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Gv_tv4w86bPb"
      },
      "outputs": [],
      "source": [
        "# Instantiating the OpenAIEmbeddings model text-embedding-3-small\n",
        "new_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "-JPe1_Jx6Rnw"
      },
      "outputs": [],
      "source": [
        "# Send the documents to the text-embedding-3-small model to get the new embeddings and load into FAISS index\n",
        "vector_store = FAISS.from_documents(documents, new_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "H-HuozNf6muZ"
      },
      "outputs": [],
      "source": [
        "# Get a retriever\n",
        "new_retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "M6Tyc3ZY7Km2"
      },
      "outputs": [],
      "source": [
        "# Create a new MultiQueryRetriever with the new retriever and the primary_qa_llm\n",
        "new_advanced_retriever = MultiQueryRetriever.from_llm(retriever=new_retriever, llm=primary_qa_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "s5QSJIhm7SKr"
      },
      "outputs": [],
      "source": [
        "new_retrieval_chain = create_retrieval_chain(new_advanced_retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "MBVjl1UK7fd7"
      },
      "outputs": [],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = new_retrieval_chain.invoke({\"input\" : question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "lTBrs0zr7iyG"
      },
      "outputs": [],
      "source": [
        "new_response_dataset_advanced_retrieval = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "86f36527c3df458aae2e54f329c643d7",
            "4aef094bb7764f4fb7917a53de5cb40a",
            "f549d2bd447649c8aac65b0abd25cf23",
            "f413d1bf4faa44edbe5d081b1d3eaff2",
            "db2d3fee6c91439faabbb891a9574392",
            "8f117fd4781949148b1533a38e29c9d6",
            "2ab3fc4aee0b456bb0a06f15de98dafd",
            "9abbc4a5bc11444185ae5ecacbfa102a",
            "f3cf4145eef74579a41f740d62d842c4",
            "21731645603f4144a74f604cf7c01021",
            "c42583faf1f3472b82394432c7623562"
          ]
        },
        "id": "hG5h-D8n7sZp",
        "outputId": "e918ff10-3631-4563-fdfb-fbab474ea87c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [01:32<00:00,  1.85s/it]\n"
          ]
        }
      ],
      "source": [
        "new_advanced_retrieval_results = evaluate(new_response_dataset_advanced_retrieval, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHdcpsZ76kj",
        "outputId": "de1bc7a3-0224-4b8a-95f8-7184fd623661"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.9815, 'answer_relevancy': 0.9493, 'context_recall': 0.9500, 'context_precision': 0.8261, 'answer_correctness': 0.7830}"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_advanced_retrieval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s4TyaCUQ79Ke",
        "outputId": "9e89e7f1-13e9-436d-e80a-c5241e1b945a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Baseline</th>\n",
              "      <th>ADA</th>\n",
              "      <th>Text Embedding 3</th>\n",
              "      <th>Delta - TE3 -&gt; ADA</th>\n",
              "      <th>Delta - TE3 -&gt; Baseline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.981481</td>\n",
              "      <td>0.048148</td>\n",
              "      <td>0.081481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.953287</td>\n",
              "      <td>0.933298</td>\n",
              "      <td>0.949339</td>\n",
              "      <td>0.016042</td>\n",
              "      <td>-0.003948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.116667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.777202</td>\n",
              "      <td>0.826111</td>\n",
              "      <td>0.048909</td>\n",
              "      <td>-0.007222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer_correctness</td>\n",
              "      <td>0.707480</td>\n",
              "      <td>0.753203</td>\n",
              "      <td>0.783001</td>\n",
              "      <td>0.029798</td>\n",
              "      <td>0.075521</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Metric  Baseline       ADA  Text Embedding 3  \\\n",
              "0        faithfulness  0.900000  0.933333          0.981481   \n",
              "1    answer_relevancy  0.953287  0.933298          0.949339   \n",
              "2      context_recall  0.950000  0.833333          0.950000   \n",
              "3   context_precision  0.833333  0.777202          0.826111   \n",
              "4  answer_correctness  0.707480  0.753203          0.783001   \n",
              "\n",
              "   Delta - TE3 -> ADA  Delta - TE3 -> Baseline  \n",
              "0            0.048148                 0.081481  \n",
              "1            0.016042                -0.003948  \n",
              "2            0.116667                 0.000000  \n",
              "3            0.048909                -0.007222  \n",
              "4            0.029798                 0.075521  "
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_baseline = pd.DataFrame(list(results.items()), columns=['Metric', 'Baseline'])\n",
        "df_original = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'ADA'])\n",
        "df_comparison = pd.DataFrame(list(new_advanced_retrieval_results.items()), columns=['Metric', 'Text Embedding 3'])\n",
        "\n",
        "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
        "df_merged = pd.merge(df_baseline, df_merged, on=\"Metric\")\n",
        "\n",
        "df_merged['Delta - TE3 -> ADA'] = df_merged['Text Embedding 3'] - df_merged['ADA']\n",
        "df_merged['Delta - TE3 -> Baseline'] = df_merged['Text Embedding 3'] - df_merged['Baseline']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRmkcMrxC4Me"
      },
      "source": [
        "####‚ùì Question #4:\n",
        "\n",
        "Do you think, in your opinion, `text-embedding-3-small` is significantly better than `ada`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yes the scores are higher across the board than ADA, and even beat the baseline on some records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOciJLABDBnA"
      },
      "source": [
        "## BONUS ACTIVITY: Showcase Multi-Context Perfomance Changes\n",
        "\n",
        "Now that we've looked at a number of different examples - showcase the difference on the multi-context *specific* questions that were synthetically generated.\n",
        "\n",
        "> NOTE: You have all the data you'll need already in the notebook if you made it to this step!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY8l2EksDH43"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002fc233bee54ea0a9729365f1e0f972": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09ce5c2f37fb469683ed7cf3bd7566f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_570a1f9809e143ef8d858e1c5dc9837d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_42c4905b54d8482588d57485c563ee78",
            "value": "Generating:‚Äá100%"
          }
        },
        "14d8c6593d6b41df8dfb290ab9f55ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384a04784f9745088478d9372161c8ae",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c981e401946b4dfca65b18b6ae56bf33",
            "value": 50
          }
        },
        "17fde9c2236b4b1b9990dd2a9fbd58ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b3b9e3adf85473a81055265d9a5b89f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "21731645603f4144a74f604cf7c01021": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2189fea4b75749d7bac330e613c31974": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2235baa0358a4b8cad60508d5d1d8380": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ee70b94d75449cbe7b7ce40ebc1049": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4b2b14a02b46c1ac67fc1581133523": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab3fc4aee0b456bb0a06f15de98dafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bf9a43c99cf4e05a0f376fde6af9ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "317a7d84efc74420abea8e311137f272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3367eaf060c845648cda48963481ecb4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384a04784f9745088478d9372161c8ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394fb069eb3c4269bb3c970cc04369a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5651973a0534d3da51d0a18b13deff3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8745b8f8f8ec46869c66758c4bc6b2e0",
            "value": "‚Äá10/10‚Äá[00:40&lt;00:00,‚Äá‚Äá6.80s/it]"
          }
        },
        "3e85b387328f4df7b45dccbe6572b9bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2189fea4b75749d7bac330e613c31974",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7148bed10a245509672dc60be6edd47",
            "value": 50
          }
        },
        "42c4905b54d8482588d57485c563ee78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4818628434aa4a0e8d7826f152c0da99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e44a47a5a2184c2780ac27e16ace0f7f",
            "max": 318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_317a7d84efc74420abea8e311137f272",
            "value": 318
          }
        },
        "4aef094bb7764f4fb7917a53de5cb40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f117fd4781949148b1533a38e29c9d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ab3fc4aee0b456bb0a06f15de98dafd",
            "value": "Evaluating:‚Äá100%"
          }
        },
        "4e5db0ff4ff44577963dbcc651ea10b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fc8cf791b1344809fe8c7ee9598a20a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50599aa481d8460aa6655330b2b0fae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfc93618fc084608bb413667fee91ea8",
              "IPY_MODEL_3e85b387328f4df7b45dccbe6572b9bd",
              "IPY_MODEL_cc50e0150a9947579a919757b85f38c9"
            ],
            "layout": "IPY_MODEL_c03d5f58d31747d3a344f813755480fc"
          }
        },
        "570a1f9809e143ef8d858e1c5dc9837d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d7c8b4640249df89b60e9eef4d2328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26ee70b94d75449cbe7b7ce40ebc1049",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc3b3593ad1e4c5bad6057a3d3872bb4",
            "value": 10
          }
        },
        "611021c94b8a42c58897925acb8b3c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6de6fb8ef2974573b50bad678620f2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09ce5c2f37fb469683ed7cf3bd7566f6",
              "IPY_MODEL_58d7c8b4640249df89b60e9eef4d2328",
              "IPY_MODEL_394fb069eb3c4269bb3c970cc04369a9"
            ],
            "layout": "IPY_MODEL_2235baa0358a4b8cad60508d5d1d8380"
          }
        },
        "83e3f8bf55454600b299fe63b608852a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a4b2b14a02b46c1ac67fc1581133523",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_decd5f4c69a845cc8fad4c21524c2fd9",
            "value": "embedding‚Äánodes:‚Äá100%"
          }
        },
        "8531b3d7f1cd424f8d3fc2e6bcd875a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b319ac9b4f1b43d5a41d6f10e6e1c1c6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_002fc233bee54ea0a9729365f1e0f972",
            "value": "‚Äá50/50‚Äá[00:21&lt;00:00,‚Äá‚Äá1.43it/s]"
          }
        },
        "86f36527c3df458aae2e54f329c643d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4aef094bb7764f4fb7917a53de5cb40a",
              "IPY_MODEL_f549d2bd447649c8aac65b0abd25cf23",
              "IPY_MODEL_f413d1bf4faa44edbe5d081b1d3eaff2"
            ],
            "layout": "IPY_MODEL_db2d3fee6c91439faabbb891a9574392"
          }
        },
        "8745b8f8f8ec46869c66758c4bc6b2e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b4c1aafe67048798cdadd46207b4b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83e3f8bf55454600b299fe63b608852a",
              "IPY_MODEL_4818628434aa4a0e8d7826f152c0da99",
              "IPY_MODEL_c3e047dfd4ec4a859e0274a54ace1432"
            ],
            "layout": "IPY_MODEL_1b3b9e3adf85473a81055265d9a5b89f"
          }
        },
        "8f117fd4781949148b1533a38e29c9d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abbc4a5bc11444185ae5ecacbfa102a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9caba03e810f4407b78cb1c1b6b9be08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af19ef64986b435c8c118e882e26f6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9caba03e810f4407b78cb1c1b6b9be08",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2bf9a43c99cf4e05a0f376fde6af9ca6",
            "value": "Evaluating:‚Äá100%"
          }
        },
        "b319ac9b4f1b43d5a41d6f10e6e1c1c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba72a1f57074488da5e34f8d02e748f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc3b3593ad1e4c5bad6057a3d3872bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c03d5f58d31747d3a344f813755480fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e047dfd4ec4a859e0274a54ace1432": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_611021c94b8a42c58897925acb8b3c5e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ba72a1f57074488da5e34f8d02e748f8",
            "value": "‚Äá318/318‚Äá[00:30&lt;00:00,‚Äá‚Äá1.69s/it]"
          }
        },
        "c42583faf1f3472b82394432c7623562": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5651973a0534d3da51d0a18b13deff3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c981e401946b4dfca65b18b6ae56bf33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc50e0150a9947579a919757b85f38c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3367eaf060c845648cda48963481ecb4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4fc8cf791b1344809fe8c7ee9598a20a",
            "value": "‚Äá50/50‚Äá[00:27&lt;00:00,‚Äá‚Äá2.66it/s]"
          }
        },
        "cfc93618fc084608bb413667fee91ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17fde9c2236b4b1b9990dd2a9fbd58ff",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ddbe87e735534504b735211253c4b4d2",
            "value": "Evaluating:‚Äá100%"
          }
        },
        "d46db515c4d543d898ef91d05df2d0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af19ef64986b435c8c118e882e26f6a9",
              "IPY_MODEL_14d8c6593d6b41df8dfb290ab9f55ca1",
              "IPY_MODEL_8531b3d7f1cd424f8d3fc2e6bcd875a0"
            ],
            "layout": "IPY_MODEL_4e5db0ff4ff44577963dbcc651ea10b8"
          }
        },
        "d7148bed10a245509672dc60be6edd47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db2d3fee6c91439faabbb891a9574392": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddbe87e735534504b735211253c4b4d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "decd5f4c69a845cc8fad4c21524c2fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e44a47a5a2184c2780ac27e16ace0f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3cf4145eef74579a41f740d62d842c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f413d1bf4faa44edbe5d081b1d3eaff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21731645603f4144a74f604cf7c01021",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c42583faf1f3472b82394432c7623562",
            "value": "‚Äá50/50‚Äá[00:22&lt;00:00,‚Äá‚Äá1.83it/s]"
          }
        },
        "f549d2bd447649c8aac65b0abd25cf23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9abbc4a5bc11444185ae5ecacbfa102a",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3cf4145eef74579a41f740d62d842c4",
            "value": 50
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
